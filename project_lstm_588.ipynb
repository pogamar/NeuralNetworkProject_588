{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pogamar/NeuralNetworkProject_588/blob/prod/project_lstm_588.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mGT18GggIl2z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f240120-dedd-4ea2-e80a-4fe5d45e8b03"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c-qUA6z6h6-V"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p-suLCcfPi7s"
      },
      "outputs": [],
      "source": [
        "#data collection\n",
        "# Load the source CSV file\n",
        "source_csv = '/content/gdrive/My Drive/project_lstm_588/1500_haskell.csv'\n",
        "df = pd.read_csv(source_csv)\n",
        "\n",
        "# Randomly select 50 rows\n",
        "df_50 = df.sample(n=10, random_state=42)\n",
        "\n",
        "# Save the new CSV file with 50 randomly selected rows\n",
        "output_csv = '/content/gdrive/My Drive/project_lstm_588/100_random_rows.csv'\n",
        "df_50.to_csv(output_csv, index=False)\n",
        "\n",
        "model_path =  '/content/gdrive/My Drive/project_lstm_588/model.pt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xk5tFe2uX4HH"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def tokenize_haskell(code):\n",
        "    # Remove single-line comments\n",
        "    code = re.sub(r'--.*', '', code)\n",
        "\n",
        "    # Remove multi-line comments\n",
        "    code = re.sub(r'{-[\\s\\S]*?-}', '', code)\n",
        "\n",
        "    # Replace string literals with a special token\n",
        "    code = re.sub(r'\\\"(?:[^\\\"\\\\]|\\\\.)*\\\"', '<STR>', code)\n",
        "\n",
        "    # Replace character literals with a special token\n",
        "    code = re.sub(r'\\'(?:[^\\'\\\\]|\\\\.)*\\'', '<CHAR>', code)\n",
        "\n",
        "    # Define a regular expression pattern for Haskell tokens\n",
        "    token_pattern = r'([(){}\\[\\]=.,;:!?|&+*\\-/<>\\^%\\$@~#]|<STR>|<CHAR>|\\b\\w+\\b)'\n",
        "\n",
        "    # Find all tokens using the regex pattern\n",
        "    tokens = re.findall(token_pattern, code)\n",
        "\n",
        "    return tokens\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SBv5-dOGpZEg"
      },
      "outputs": [],
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self, dataset):\n",
        "        super(Model, self).__init__()\n",
        "        self.lstm_size = 128\n",
        "        self.hidden_size = 128\n",
        "        self.embedding_dim = 128\n",
        "        self.num_layers = 3\n",
        "\n",
        "        n_vocab = len(dataset.uniq_words)\n",
        "        self.embedding = nn.Embedding(\n",
        "            num_embeddings=n_vocab,\n",
        "            embedding_dim=self.embedding_dim,\n",
        "        )\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=self.lstm_size,\n",
        "            hidden_size=self.hidden_size,\n",
        "            num_layers=self.num_layers,\n",
        "            dropout=0.2,\n",
        "        )\n",
        "        self.fc = nn.Linear(self.lstm_size, n_vocab)\n",
        "\n",
        "    def forward(self, x, prev_state):\n",
        "        embed = self.embedding(x)\n",
        "        output, state = self.lstm(embed, prev_state)\n",
        "        logits = self.fc(output)\n",
        "        return logits, state\n",
        "\n",
        "    def init_state(self, sequence_length):\n",
        "        return (torch.zeros(self.num_layers, sequence_length, self.lstm_size),\n",
        "                torch.zeros(self.num_layers, sequence_length, self.lstm_size))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YdUlD5GW0cLB"
      },
      "outputs": [],
      "source": [
        "class HaskellDatasetPre(torch.utils.data.Dataset):\n",
        "    def __init__(self, args, vocab=None):\n",
        "        self.args = args\n",
        "        self.words = self.load_words()\n",
        "        if vocab:\n",
        "            self.uniq_words = vocab\n",
        "        else:\n",
        "            self.uniq_words = self.get_uniq_words()\n",
        "\n",
        "        self.index_to_word = {index: word for index, word in enumerate(self.uniq_words)}\n",
        "        self.word_to_index = {word: index for index, word in enumerate(self.uniq_words)}\n",
        "\n",
        "        self.words_indexes = [self.word_to_index[w] for w in self.words]\n",
        "\n",
        "    def load_words(self):\n",
        "        #data collection\n",
        "        df = pd.read_csv(source_csv)\n",
        "        tokenized_sequences = [tokenize_haskell(snippet) for snippet in list(df.content.values)]\n",
        "\n",
        "        words = []\n",
        "        for tokens in tokenized_sequences:\n",
        "            words.extend(tokens)\n",
        "        return words\n",
        "\n",
        "    def get_uniq_words(self):\n",
        "        word_counts = Counter(self.words)\n",
        "        return sorted(word_counts, key=word_counts.get, reverse=True)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.words_indexes) - self.args.sequence_length\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return (\n",
        "            torch.tensor(self.words_indexes[index:index+self.args.sequence_length]),\n",
        "            torch.tensor(self.words_indexes[index+1:index+self.args.sequence_length+1]),\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p93dUfXosgdX"
      },
      "outputs": [],
      "source": [
        "class HaskellDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, args, vocab=None):\n",
        "        self.args = args\n",
        "        self.words = self.load_words()\n",
        "        if vocab:\n",
        "            self.uniq_words = vocab\n",
        "        else:\n",
        "            self.uniq_words = self.get_uniq_words()\n",
        "\n",
        "        self.index_to_word = {index: word for index, word in enumerate(self.uniq_words)}\n",
        "        self.word_to_index = {word: index for index, word in enumerate(self.uniq_words)}\n",
        "\n",
        "        self.words_indexes = [self.word_to_index[w] for w in self.words]\n",
        "\n",
        "    def load_words(self):\n",
        "        #data collection\n",
        "        df = pd.read_csv(output_csv)\n",
        "        tokenized_sequences = [tokenize_haskell(snippet) for snippet in list(df.content.values)]\n",
        "\n",
        "        words = []\n",
        "        for tokens in tokenized_sequences:\n",
        "            words.extend(tokens)\n",
        "        return words\n",
        "\n",
        "    def get_uniq_words(self):\n",
        "        word_counts = Counter(self.words)\n",
        "        return sorted(word_counts, key=word_counts.get, reverse=True)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.words_indexes) - self.args.sequence_length\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return (\n",
        "            torch.tensor(self.words_indexes[index:index+self.args.sequence_length]),\n",
        "            torch.tensor(self.words_indexes[index+1:index+self.args.sequence_length+1]),\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cwlSh6x7wPEa"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import torch\n",
        "import numpy as np\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def train(dataset, model, args, device):\n",
        "    model.train()\n",
        "\n",
        "    dataloader = DataLoader(dataset, batch_size=args.batch_size, num_workers=2, pin_memory=True)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "    losses = []\n",
        "\n",
        "    for epoch in range(args.max_epochs):\n",
        "      state_h, state_c = model.init_state(args.sequence_length)\n",
        "\n",
        "      for batch, (x, y) in enumerate(dataloader):\n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "        # Move the hidden tensors to the same device as the input tensor\n",
        "        state_h, state_c = state_h.to(device), state_c.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        y_pred, (state_h, state_c) = model(x, (state_h, state_c))\n",
        "        loss = criterion(y_pred.transpose(1, 2), y)\n",
        "        state_h = state_h.detach()\n",
        "        state_c = state_c.detach()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch % 100 == 0:\n",
        "          print({ 'epoch': epoch, 'batch': batch, 'loss': loss.item() })\n",
        "          losses.append(loss.item())\n",
        "\n",
        "      torch.save(model.state_dict(), model_path)\n",
        "    return losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PAgvH-94yHDC"
      },
      "outputs": [],
      "source": [
        "def generate_code_from_source(dataset, model, input_seq):\n",
        "    generated_code = predict(dataset, model, text=input_seq, next_words=50)\n",
        "    return generated_code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I9DoTuSdwh80"
      },
      "outputs": [],
      "source": [
        "def predict(dataset, model, text, next_words=100):\n",
        "    model.eval()\n",
        "\n",
        "    device = next(model.parameters()).device \n",
        "\n",
        "    words = text.split(' ')\n",
        "    state_h, state_c = model.init_state(len(words))\n",
        "\n",
        "    state_h = state_h.to(device) \n",
        "    state_c = state_c.to(device) \n",
        "\n",
        "    for i in range(0, next_words):\n",
        "        x = torch.tensor([[dataset.word_to_index[w] for w in words[i:]]]).to(device)\n",
        "        y_pred, (state_h, state_c) = model(x, (state_h, state_c))\n",
        "\n",
        "        last_word_logits = y_pred[0][-1]\n",
        "        p = torch.nn.functional.softmax(last_word_logits, dim=0).detach().cpu().numpy()\n",
        "        word_index = np.random.choice(len(last_word_logits), p=p)\n",
        "        words.append(dataset.index_to_word[word_index])\n",
        "\n",
        "    return words\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5mFRskYsxpjl"
      },
      "outputs": [],
      "source": [
        "class Args:\n",
        "    def __init__(self, max_epochs, batch_size, sequence_length):\n",
        "        self.max_epochs = 10\n",
        "        self.batch_size = 256\n",
        "        self.sequence_length = 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZrerfSFywlas",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0804ae3b-098e-4944-e680-876285d83e0a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model loaded\n",
            "{'epoch': 0, 'batch': 0, 'loss': 3.818061590194702}\n"
          ]
        }
      ],
      "source": [
        "from os import write\n",
        "args =Args(max_epochs=1, batch_size=500, sequence_length=50)\n",
        "\n",
        "training_dataset = HaskellDatasetPre(args)\n",
        "\n",
        "# Then, get the vocabulary list from the training_dataset\n",
        "training_vocabulary_list = training_dataset.uniq_words\n",
        "\n",
        "dataset = HaskellDataset(args, vocab=training_vocabulary_list)\n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
        "\n",
        "\n",
        "try:\n",
        "  model_state_dict = torch.load(model_path)\n",
        "except:\n",
        "  model_state_dict = None\n",
        "model = Model(dataset)\n",
        "if model_state_dict :\n",
        "  print(\"model loaded\")\n",
        "  model.load_state_dict(model_state_dict)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "#train(dataset, model, args)\n",
        "losses = train(train_dataset, model, args, device)\n",
        "lossdf = pd.DataFrame(losses)\n",
        "lossdf.to_csv(\"loss.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "prZmBn311acN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7bf20d6-daf1-4706-f8cc-477f5d700fe9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['let', 'input', 'rnf', 'c', ':', 'stack', '(', 'mkName', '<', 'STR', '>', ')', '=', 'd', '=', 'const', '<', 'STR', '>', ',', '(', 'mkName', '<', 'STR', '>', '+', 'x', 'y', '_', 'y', 'Text', 'import', 'Control', '.', 'map', '(', '(', 'mkName', '<', 'STR', '>', '<', 'STR', '>', '<', 'STR', '>', 'e', 'else', 'x', ',', 'qClient', ',', 'all', '=', 'f', '.', 'Char', ')', ',', '(', 'State', '(', 'NoArg', 't', 'k', 'arg', '=', 'findWithDefault', 'Right', '$', 'TRow', '(', 'mkName', '<', '*', '>', ':', 'Types', 'instance', 'FromReqURI', '}', ')', 'l2', '(', 'Mirror', '(', 'x', '+', 'show', '(', '<', '-', '>', ')', ')', '|', '<', 'STR', '>', '=']\n",
            "Average BLEU score 0.007409673891109517\n"
          ]
        }
      ],
      "source": [
        "print(predict(dataset, model, text='let'))\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from nltk.translate.bleu_score import SmoothingFunction\n",
        "\n",
        "bleu_scores = []\n",
        "for idx in range(len(val_dataset)):\n",
        "  input_source, target_code = val_dataset[idx]\n",
        "  input_words = ' '.join([val_dataset.dataset.index_to_word[token.item()] for token in input_source])\n",
        "  generated_words = generate_code_from_source(val_dataset.dataset, model, input_words, device)\n",
        "  target_words = ' '.join([val_dataset.dataset.index_to_word[token.item()] for token in target_code])\n",
        "  bleu_score = sentence_bleu([target_words.split()], generated_words, smoothing_function=SmoothingFunction().method1)\n",
        "  bleu_scores.append(bleu_score)\n",
        "avg_bleu_score = sum(bleu_scores) / len(bleu_scores)\n",
        "print('Average BLEU score {}'.format( avg_bleu_score))\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOTagXDBTOtVGt7QnbLvSi3",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}