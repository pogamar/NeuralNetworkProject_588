{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pogamar/NeuralNetworkProject_588/blob/prod/project_lstm_588.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mGT18GggIl2z",
        "outputId": "cab35102-2e32-4327-89b9-7877f89463d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "c-qUA6z6h6-V"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "p-suLCcfPi7s"
      },
      "outputs": [],
      "source": [
        "#data collection\n",
        "# Load the source CSV file\n",
        "source_csv = '/content/gdrive/My Drive/project_lstm_588/1500_haskell.csv'\n",
        "df = pd.read_csv(source_csv)\n",
        "\n",
        "# Randomly select 50 rows\n",
        "df_50 = df.sample(n=100, random_state=42)\n",
        "\n",
        "# Save the new CSV file with 50 randomly selected rows\n",
        "output_csv = '/content/gdrive/My Drive/project_lstm_588/100_random_rows.csv'\n",
        "df_50.to_csv(output_csv, index=False)\n",
        "\n",
        "model_path =  '/content/gdrive/My Drive/project_lstm_588/model.pt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "xk5tFe2uX4HH"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def tokenize_haskell(code):\n",
        "    # Remove single-line comments\n",
        "    code = re.sub(r'--.*', '', code)\n",
        "\n",
        "    # Remove multi-line comments\n",
        "    code = re.sub(r'{-[\\s\\S]*?-}', '', code)\n",
        "\n",
        "    # Replace string literals with a special token\n",
        "    code = re.sub(r'\\\"(?:[^\\\"\\\\]|\\\\.)*\\\"', '<STR>', code)\n",
        "\n",
        "    # Replace character literals with a special token\n",
        "    code = re.sub(r'\\'(?:[^\\'\\\\]|\\\\.)*\\'', '<CHAR>', code)\n",
        "\n",
        "    # Define a regular expression pattern for Haskell tokens\n",
        "    token_pattern = r'([(){}\\[\\]=.,;:!?|&+*\\-/<>\\^%\\$@~#]|<STR>|<CHAR>|\\b\\w+\\b)'\n",
        "\n",
        "    # Find all tokens using the regex pattern\n",
        "    tokens = re.findall(token_pattern, code)\n",
        "\n",
        "    return tokens\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "SBv5-dOGpZEg"
      },
      "outputs": [],
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self, dataset):\n",
        "        super(Model, self).__init__()\n",
        "        self.lstm_size = 128\n",
        "        self.hidden_size = 256\n",
        "        self.embedding_dim = 128\n",
        "        self.num_layers = 3\n",
        "\n",
        "        n_vocab = len(dataset.uniq_words)\n",
        "        self.embedding = nn.Embedding(\n",
        "            num_embeddings=n_vocab,\n",
        "            embedding_dim=self.embedding_dim,\n",
        "        )\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=self.lstm_size,\n",
        "            hidden_size=self.hidden_size,\n",
        "            num_layers=self.num_layers,\n",
        "            dropout=0.2,\n",
        "        )\n",
        "        self.fc = nn.Linear(self.lstm_size, n_vocab)\n",
        "\n",
        "    def forward(self, x, prev_state):\n",
        "        embed = self.embedding(x)\n",
        "        output, state = self.lstm(embed, prev_state)\n",
        "        logits = self.fc(output)\n",
        "        return logits, state\n",
        "\n",
        "    def init_state(self, sequence_length):\n",
        "        return (torch.zeros(self.num_layers, sequence_length, self.lstm_size),\n",
        "                torch.zeros(self.num_layers, sequence_length, self.lstm_size))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "YdUlD5GW0cLB"
      },
      "outputs": [],
      "source": [
        "class HaskellDatasetPre(torch.utils.data.Dataset):\n",
        "    def __init__(self, args, vocab=None):\n",
        "        self.args = args\n",
        "        self.words = self.load_words()\n",
        "        if vocab:\n",
        "            self.uniq_words = vocab\n",
        "        else:\n",
        "            self.uniq_words = self.get_uniq_words()\n",
        "\n",
        "        self.index_to_word = {index: word for index, word in enumerate(self.uniq_words)}\n",
        "        self.word_to_index = {word: index for index, word in enumerate(self.uniq_words)}\n",
        "\n",
        "        self.words_indexes = [self.word_to_index[w] for w in self.words]\n",
        "\n",
        "    def load_words(self):\n",
        "        #data collection\n",
        "        df = pd.read_csv(source_csv)\n",
        "        tokenized_sequences = [tokenize_haskell(snippet) for snippet in list(df.content.values)]\n",
        "\n",
        "        words = []\n",
        "        for tokens in tokenized_sequences:\n",
        "            words.extend(tokens)\n",
        "        return words\n",
        "\n",
        "    def get_uniq_words(self):\n",
        "        word_counts = Counter(self.words)\n",
        "        return sorted(word_counts, key=word_counts.get, reverse=True)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.words_indexes) - self.args.sequence_length\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return (\n",
        "            torch.tensor(self.words_indexes[index:index+self.args.sequence_length]),\n",
        "            torch.tensor(self.words_indexes[index+1:index+self.args.sequence_length+1]),\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "p93dUfXosgdX"
      },
      "outputs": [],
      "source": [
        "class HaskellDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, args, vocab=None):\n",
        "        self.args = args\n",
        "        self.words = self.load_words()\n",
        "        if vocab:\n",
        "            self.uniq_words = vocab\n",
        "        else:\n",
        "            self.uniq_words = self.get_uniq_words()\n",
        "\n",
        "        self.index_to_word = {index: word for index, word in enumerate(self.uniq_words)}\n",
        "        self.word_to_index = {word: index for index, word in enumerate(self.uniq_words)}\n",
        "\n",
        "        self.words_indexes = [self.word_to_index[w] for w in self.words]\n",
        "\n",
        "    def load_words(self):\n",
        "        #data collection\n",
        "        df = pd.read_csv(output_csv)\n",
        "        tokenized_sequences = [tokenize_haskell(snippet) for snippet in list(df.content.values)]\n",
        "\n",
        "        words = []\n",
        "        for tokens in tokenized_sequences:\n",
        "            words.extend(tokens)\n",
        "        return words\n",
        "\n",
        "    def get_uniq_words(self):\n",
        "        word_counts = Counter(self.words)\n",
        "        return sorted(word_counts, key=word_counts.get, reverse=True)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.words_indexes) - self.args.sequence_length\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return (\n",
        "            torch.tensor(self.words_indexes[index:index+self.args.sequence_length]),\n",
        "            torch.tensor(self.words_indexes[index+1:index+self.args.sequence_length+1]),\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "cwlSh6x7wPEa"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import torch\n",
        "import numpy as np\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def train(dataset, model, args):\n",
        "    model.train()\n",
        "\n",
        "    dataloader = DataLoader(dataset, batch_size=args.batch_size, num_workers=2)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    for epoch in range(args.max_epochs):\n",
        "      state_h, state_c = model.init_state(args.sequence_length)\n",
        "\n",
        "      for batch, (x, y) in enumerate(dataloader):\n",
        "        optimizer.zero_grad()\n",
        "        y_pred, (state_h, state_c) = model(x, (state_h, state_c))\n",
        "        loss = criterion(y_pred.transpose(1, 2), y)\n",
        "        state_h = state_h.detach()\n",
        "        state_c = state_c.detach()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch % 100 == 0:\n",
        "          print({ 'epoch': epoch, 'batch': batch, 'loss': loss.item() })\n",
        "\n",
        "      torch.save(model.state_dict(), model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "PAgvH-94yHDC"
      },
      "outputs": [],
      "source": [
        "def generate_code_from_source(dataset, model, input_seq):\n",
        "    generated_code = predict(dataset, model, text=input_seq, next_words=50)\n",
        "    return generated_code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "I9DoTuSdwh80"
      },
      "outputs": [],
      "source": [
        "def predict(dataset, model, text, next_words=100):\n",
        "    model.eval()\n",
        "\n",
        "    words = text.split(' ')\n",
        "    state_h, state_c = model.init_state(len(words))\n",
        "\n",
        "    for i in range(0, next_words):\n",
        "        x = torch.tensor([[dataset.word_to_index[w] for w in words[i:]]])\n",
        "        y_pred, (state_h, state_c) = model(x, (state_h, state_c))\n",
        "\n",
        "        last_word_logits = y_pred[0][-1]\n",
        "        p = torch.nn.functional.softmax(last_word_logits, dim=0).detach().numpy()\n",
        "        word_index = np.random.choice(len(last_word_logits), p=p)\n",
        "        words.append(dataset.index_to_word[word_index])\n",
        "\n",
        "    return words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "5mFRskYsxpjl"
      },
      "outputs": [],
      "source": [
        "class Args:\n",
        "    def __init__(self, max_epochs, batch_size, sequence_length):\n",
        "        self.max_epochs = 10\n",
        "        self.batch_size = 256\n",
        "        self.sequence_length = 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZrerfSFywlas",
        "outputId": "d465f237-97c1-49fa-d6a3-da452a7ba70d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model loaded\n",
            "{'epoch': 0, 'batch': 0, 'loss': 0.47985339164733887}\n",
            "{'epoch': 1, 'batch': 0, 'loss': 0.4477563202381134}\n",
            "{'epoch': 2, 'batch': 0, 'loss': 0.38539692759513855}\n",
            "{'epoch': 3, 'batch': 0, 'loss': 0.3667314052581787}\n",
            "{'epoch': 4, 'batch': 0, 'loss': 0.3358776867389679}\n",
            "{'epoch': 5, 'batch': 0, 'loss': 0.3535510003566742}\n",
            "{'epoch': 6, 'batch': 0, 'loss': 0.34082895517349243}\n",
            "{'epoch': 7, 'batch': 0, 'loss': 0.31231504678726196}\n",
            "{'epoch': 8, 'batch': 0, 'loss': 0.3088725507259369}\n",
            "{'epoch': 9, 'batch': 0, 'loss': 0.300496369600296}\n"
          ]
        }
      ],
      "source": [
        "from zmq.constants import NULL\n",
        "\n",
        "args =Args(max_epochs=5, batch_size=500, sequence_length=50)\n",
        "\n",
        "training_dataset = HaskellDatasetPre(args)\n",
        "\n",
        "# Then, get the vocabulary list from the training_dataset\n",
        "training_vocabulary_list = training_dataset.uniq_words\n",
        "\n",
        "dataset = HaskellDataset(args, vocab=training_vocabulary_list)\n",
        "\n",
        "try:\n",
        "  model_state_dict = torch.load('/content/gdrive/My Drive/project_lstm_588/model.pt')\n",
        "except:\n",
        "  model_state_dict = NULL\n",
        "model = Model(dataset)\n",
        "if model_state_dict :\n",
        "  print(\"model loaded\")\n",
        "  model.load_state_dict(model_state_dict)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "train(dataset, model, args)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "prZmBn311acN"
      },
      "outputs": [],
      "source": [
        "print(predict(dataset, model, text='let'))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM6q1SmfQCK7n4cIKjy6L8L",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}